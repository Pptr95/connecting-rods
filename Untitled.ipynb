{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image formation and acquisition\n",
    "\n",
    "- pinhole camera\n",
    "- perspective projection\n",
    "    - its properties\n",
    "    - perspective projection equations $u$ and $v$. What these equations tell us?\n",
    "- standard stereo\n",
    "- epipolar geometry\n",
    "- vanishing point\n",
    "- lenses. Why pinhole camera model is not ok in practice?\n",
    "    - thin lense equation\n",
    "    - circle of confusion (or blur circles)\n",
    "    - diaphragm\n",
    "- what is noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity transformations\n",
    "- what is the instensity transformation or point operator\n",
    "- histogram\n",
    "- linear contrast streching\n",
    "- histogram equalization and advantages wrt linear contrast stetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial filtering\n",
    "- spatial filtering definition\n",
    "- LSI operator properties\n",
    "- convolution\n",
    "    - properties\n",
    "- correlation\n",
    "    - properties\n",
    "    - difference wrt convolution\n",
    "- mean filter\n",
    "    - advantages and disadvanatges\n",
    "    - how to speed up mean filter? Box-filtering\n",
    "- noise\n",
    "    - gaussian\n",
    "    - impulse\n",
    "- gaussian filter\n",
    "    - why is better for smoothing wrt mean filter? When instead is better using mean? (computation)\n",
    "    - practical implementation\n",
    "    - rule of thumb for the kernel size\n",
    "    - separability\n",
    "- median filter (non-linear)\n",
    "    - what to do when we have both impulse an gaussian noise?\n",
    "- bilateral filter (non-linear)\n",
    "- non local means filter (non-linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image segmentation\n",
    "- definition\n",
    "- Otsu's algorithm\n",
    "    - how to speed up\n",
    "- Mahalanobis distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Morphology\n",
    "- definition\n",
    "- dilation and erosion\n",
    "- opening and closing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blob Analysis\n",
    "- definition\n",
    "- distances types\n",
    "- connected componet\n",
    "- CCL\n",
    "- area, perimeter, barycentre\n",
    "- compactness, Heralick's Circularity\n",
    "- moments\n",
    "    - definition / what they are used for\n",
    "    - moments types\n",
    "    - central moments for traslation and scale invariance\n",
    "- orientation\n",
    "    - major axis\n",
    "    - finding major axis\n",
    "        - l: aj + bi + c = 0, p = [i j]T\n",
    "        - $d_l^2$\n",
    "        - two semplifications\n",
    "            - reference system from camera to object reference system (baryceter)\n",
    "            - consider the orientation of the line **I** as a uniary vector which has the same orientation of the line\n",
    "        - **I** = [-sin($\\theta$) cos($\\theta$)] = [alpha beta]\n",
    "        - M(**I**) = sum(alpha j - beta i)^2\n",
    "        - extend the previous formula and find three moments\n",
    "        - substitute alpha and beta and rewrite M wrt $\\theta$\n",
    "        - first derivate and second derivative\n",
    "    - minor axis\n",
    "    - MER\n",
    "        - its features\n",
    "        - how to achieve invariance to rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge detection\n",
    "- gradient of an image\n",
    "- noise\n",
    "    - smooth + derivatives problem\n",
    "    - smooth derivates (single step)\n",
    "- prewitt\n",
    "- sobel\n",
    "- NMS\n",
    "- LOG\n",
    "    - initial idea of zero crossing of second order derivatative (too computation)\n",
    "    - so, use an approximation for caculate second order derivative -> Laplacian\n",
    "    - this would not work due to noise, so Gaussian smooth first\n",
    "    - find zero crossing\n",
    "    - 3w <= d <= 4w with d size of Mexican Hat\n",
    "    - separability to speed up\n",
    "- Canny's edge detection\n",
    "    - three criteria\n",
    "        - good detection\n",
    "        - good localization\n",
    "        - one response to one edge\n",
    "    - gaussian smoothing + compute gradient (one step)\n",
    "    - NMS\n",
    "    - one Th, with changes of light, cannot handle well the edge detector\n",
    "        - hysteresis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera calibration\n",
    "- point at infinity in Euclidean Space\n",
    "- point at infinity in Projective Space (this representation is called homogeneous coordinates)\n",
    "    - add 1 and multiply by k != 0. If k == 0 this is the point at infinity\n",
    "    - to go back to Euclidean Space, divide by k and delete the fourth coordinate (if is 0, we cannot go back)\n",
    "- PP from Euclidean to Projective Space\n",
    "    - **km = PM**, from non-linear to linear, where **M** is the 3D point expressed in CRF, **m** is its projection onto the image plane and **P** is the PPM.\n",
    "    - how to find vanishing point in this new space\n",
    "        - suppose our vector in Euclidean space being [a b c], we transform into PP space so it becomes [a b c 1]\n",
    "        - since the VP is the point at infinity, we put 0 as fourth coordinate, so [a b c 0]\n",
    "        - now we apply the formula km = PM, so km is then [fa fb c]\n",
    "        - to get the VP in Euclidean space, we divide by c and delete third coordinate <br>\n",
    "        \n",
    "        - **EX**:find VP of the lines **parallel to z** axis. So we project [0 0 1] (cos(90) = 0 and cos(0) = 1)\n",
    "        - we will find that is the center of the image [0 0]\n",
    "        \n",
    "        - **EX**:find VP of the lines **parallel to the image plane**- So we project [a b 0].\n",
    "        - we get km equal to [af bf 0], since the third coordinate is zero we cannot go back in Euclidean space\n",
    "- Intrinsic parameters\n",
    "    - are the characteristics of the image sensing devices (it encodes information regarding the digitaization and the translation of the piercing point, the interesection between the optical axis and the image plane).\n",
    "- Estrinsic parameters    \n",
    "    - encodes the position and orientation of the camera wrt WRF\n",
    "- Camera calibration\n",
    "    - camera calibration is the process whereby we get to know all the parameter of the PPM. So we get to know the intrinsic, extrinsic and lens distorsion parameters.\n",
    "    - to solve the linear equations to find the PPM parameters, we should know the correspondeces between world points and pixel points, such that, once we know, we can solve for the parameters\n",
    "    - how to find correspondeces? We use **calibration targets**\n",
    "    - Zhang's Method\n",
    "        - **1- Acquire *n* images of a planar pattern with *m* internal coners.**\n",
    "            - In this first step we acquire *n* images with a planar target (even though different types of targets can be used). In each image, the 3D WRF is taken at the top-left corner of the pattern with plane *z* = 0. The third coordinate will always be 0 and *x* and *y* are determined by the known size of the chessboard squares.\n",
    "        - **2- For each such image compute an initial guess for homography** $H_i$.\n",
    "            - Since we consider a reference system with the *z* coordinate set to 0, the PPM boils down to be a **homography**. Such transformation is known as homography and represents a general linear transformation between planes. <br> So now our equation is **km = Hw'**, where:\n",
    "                - **m** is a vector containing the pixel coordinates of a corner\n",
    "                - **w'** is a vector containing the world coordinates of the control point (a point in the chessboard)\n",
    "                - **H** is the homography I am willing to estimate\n",
    "            - based on the upper equation and supposing we are now in R3 (Euclidean domain), we know that **m x Hw = 0**, namely their vector product is zero becuse in R3 they are orthogonal vectors. So we can take advantage of this to setup the system we need to solve for the unknown homography parameters based on the **DLT** algorithm (directly linear transform). The solution of this system is basically a minimization of the **algebraic error** which actually has no physical meanining but is just a mathematical entity we seek to minimize because we formulated the system in that way.\n",
    "            \n",
    "        - **3- Refine each H by minimizing the geometric error.**\n",
    "            - To find a good homography, what should we minimize in reality? Remember that the homography is going to predict where a world point is gonna be found within the image. If I have estimated an H and I multiply this H by the coordinates of a world point I get the image point. So what we are trying to minimize is how good was my prediction. Namely, the prediction is good when I take the corner point represented by Hw and this is exactly m.\n",
    "\n",
    "            - So, given the previous initial estimation, **H** is later refined in a least-square sense through this minimization problem:\n",
    "                - min H (sum ||m - Hw||^2)\n",
    "            - the upper minimization problem is the sum of Euclidean distances between the observed corners and the projected corners according to the estimated homography. And this is done for each corner in the image. The error we are minimization here is also known as **geometric error**.\n",
    "        - **4- Get an initial guess for estrinsic parameters given the estimated homographies H.**\n",
    "        - **5- Get an initial guess for intrinsic parameters.**\n",
    "        - **6- Compute an initial guess for lens distorsion parameters.**\n",
    "            - It is important to underline that the distorsion effect is applied **before** the pixalization.\n",
    "        - **7- Refinement by non linear optimization.**\n",
    "            - In the procedure performed so far we sought to minimize an algebraic error, without any real physical meaning. At the end, we do a final optimization of the estimation of parameters performed so far for returning the optimal parameters.\n",
    "- image warping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local invariants features\n",
    "- performance of the matching process\n",
    "    - **recall**: TP / P, recall tells me how many truly keypoints I matched among all the matchable keypoints\n",
    "    - **precision**: TP / TP + TF, precision tells me how many of my the matches are gonna be correct\n",
    "- Stephens Harris corner detector\n",
    "    -  Consider taking an image patch $w(x, y)$ and shifting it across the image by ($\\nabla_x$, $\\nabla_y$), The sum of squared differences (SSD) between these two patches can be written, after some calculations, in this form:\n",
    "    <br>\n",
    "    $E$($\\nabla_x$, $\\nabla_y$) = [$\\nabla_x$, $\\nabla_y$] $M$ [($\\nabla_x$, $\\nabla_y$)]T <br>\n",
    "    where $M$ encodes the local image structure around the considered pixel. Then, hypotesizing M is a diagnal matrix, we can use SVD to calculate its eigenvalues $\\lambda_1$ and $\\lambda_2$. In order to be a corner, these two lambda must be both high. We can **speed up** we can avoid calulating the eigenvalues since it is expensive and we can take advantage by the determinant of the matrix and trace and compute a **cornerness** function based on that. It is rotation invariance **but not** scale invariance.\n",
    "    \n",
    "- DoG detector\n",
    "    - David Lowe proposed to detect keypoints by seeking for the extrema of the DoG (Difference of Gaussian) function across the $(x, y, \\sigma)$ domain. DoG$(x, y, \\sigma)$ = $(G(x, y, k\\sigma) - G(x, y, \\sigma)) * I(x, y)$ = $L(x, y, k\\sigma) - L(x, y, \\sigma)$\n",
    "    - We start by smoothing with a certain sigma, then with another and another one. We do that for a range in which $\\sigma$ gets doubled. This range of scales in which gets doubled is called **octave**. If we want to split an octave into 4 intervals, we can set up an equation and finding the value of k. In general we can have more intervals, it is a parameter. Once we have computed the Gaussian with sigma and the Gaussian with sigma, we compute the DoG, namely take the difference between the Gaussian smoothed images. After we have our stack of DoG images we look at this stack to find the extrema (which are exactly our features). To find the extrema, at each level, we look in the above DoG image and below. If a pixel is either the highest or the smallest, we select that pixel to be a feature point. We cannot do that for the toppest and lowest image so we add two Gaussian filtered images smoothed by multypling by k the toppest and divinding by k the lowest. All this process concern one octave, but we want more octaves, if we want to build a second octave we should start with $2\\sigma$. This is possible however expensive. So instead of growing sigma squeeze accordingly the image.\n",
    "    \n",
    "- SIFT\n",
    "    - if we compute a descriptor considering a patch around the keypoints (its neigborhood) this will not provide a invariant to rotation descriptor. So the patch should be oriented alongside its canonical orientation. The canonical orietation of a patch around the keypoint is computed basically by finding the dominant orientation of the gradients in the neigbourhood patch. This is achieved by discretize the \"circle\" and then construct an orientation histogram where each bin is a gradient direction and the histogram is incremented by the magnitude of the gradient. The dominat orientation (characteristic orientation) is given by the highest peak in the histogram. Also, more other peaks may be taken, so keypoints could have multiple canonical orientations. Doing to, then the patch we will consider around the keypoint will be oriented as the canonical orientation and so will be rotation invariant. Then all the pixels in the oriented patch will ve used to compute the descriptor. Also, to compute the above gradient orientation histogram, the pixels closer to the keypoint has an higher weight. This is achieved by a Gaussian function.\n",
    "    - now we are ready to compute the descriptor. For each keypoint $(x, y, \\sigma)$ we take around it a 16x16 pixel grid in its associated Gaussian smoothed image with the very same sigma (degree of scale). The grid id further divided into 4x4 regions each of size 4x4 pixels and a gradient orientation histogram is created for each region (same process as before). In this case, to avoid boundary effects, a soft rather than hard assignment in empolyed during the gradient histogram orientation (if the gradient is near two bin, its contribution is proportionally split between the two bin based in the distance.\n",
    "    -  At the end the SIFT descriptor has a size equal to the number of regions x number of histogram bins per region, i.e. 4x4x8 = 128 elements\n",
    "    \n",
    "- Matching descriptors\n",
    "    - now, we match keypoints with descriptors. How do we match descriptors? In SIFT we use Euclidean distance. In particulare NN Search is employed. For declara a match, two criteria are taken into account:\n",
    "        1- d_nn < = T\n",
    "        2- d_nn / d_2_nn <= T (here we want a small value / large value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection\n",
    "- dissimilarity functions\n",
    "    - SSD\n",
    "    - SAD\n",
    "    - NCC, cosine similarity, it is invariant to intensity changes however is more computationally expensive. When the light is controllable, SSD or SAD are preferable\n",
    "    - ZNCC, zero-mean normalized cross correlation. ZNCC is the same as NCC but is computed by subtracting the means of the subimage and the template\n",
    "\n",
    "- Bound-based methods\n",
    "    - let D(i, j) be the dissimilarity function to be minimized to detect T and assume there exist a lower bound: B(i, j) <= D(i, j)\n",
    "    - let D_min being the smallest D found so far\n",
    "    - having the above relationship we can compute beta instad of D. If beta turns out to be larger than D_min, we can conslude that there is no chance for beta to be smaller than D_min. So we **skip** the computation of D.\n",
    "    - this kind of approach will make sense when computing beta is much faster than D (**efficient bound**). Moreover it must be the case that that condition is satisfied many times otherwise we will do more computation because we will compute both beta and D (for example when beta is -inf). The condition is said to be **effective bound**\n",
    "    - we know that beta <= SAD/SSD because we take advantage of the **triangle inequality** to compute off-line and once and for all the computation T and the computation of I is done thorugh box filtering. So the above relationship will hold.\n",
    "\n",
    "- shape-based matching\n",
    "    - first a set of control points are extracted from the template by edge detection and we store the gradient orientation of those control points\n",
    "    - then, at each position of the target image, the stored gradients of the template at keypoints are compared to the sub image gradients and we want (to declare a match) to be ad aligned as possible\n",
    "    - this approach is good because is not affected by intensity changes gain since we compare gradient directions and not gradient magnitude (which is affected). Moreover, we are not going to apply edge detection in the sub image which will be a problem with light intensity because the chosen threshold could be not good anymore. We just compute gradient directions and compare them\n",
    "    - the similarity function used is a \"normalized cosine similarity\" which output range is [-1, 1]\n",
    "    - we can be invariance wrt inversion of contrast polarity applying the modulo to the similarity function\n",
    "  \n",
    "- Hough transform HT\n",
    "    - HT is a way to find shapes which we can define through an equation. The idea is that we do not search for the target shape within the image, but we search into an auxiliary space which is called **parameter** space of that shape. The HT is applied after an edge detectin, so the input data consist of the edge pixels extracted from the original image. So with this king of transformation, for example 2 points in the image space are represented through lines in the parameter space.\n",
    "    - intersections of curves or line in the parameter space indicate the presence of that shape we are looking for within the image. The more intersection, the more likely\n",
    "    - in practise these lines in the parameter space have to be quantized and stored in an array called **Accumulator Array (AA)**. So rather than drawing continuous lines, we count how many lines fall into a specific cell. Then we declare a detection when we find a cell with a high number of, so called, votes (cell with high peak).\n",
    "    - since the parameters of a shape has an infinte range, we create a new representation called **normal parametrization** which has a finite range and here the image points are mapped into sinusoidal curves\n",
    "    - it is robus to **occlusion** because we declare a detection where there is an high value into the AA, that means that there are enough pixels that are compatible with a particulat shape. If there is occlusion in the image, it is likely that in the AA we still have peaks for that shape (for a certain degree of occlusion)\n",
    "    - it is robus to **noise** becuase the noise is going to take a role only in edge detection process. This noisy edges are going to vote randomly then; is very unlikely that all the noisy edges are gonna be arranged according to a particular shape, so they are going to vote randomly in the AA and this is not a problem as far seeking for peaks is concerned\n",
    "    \n",
    "- Generalized Hough transform GHT\n",
    "    - GHT is the extension of HT to detect arbitrary shapes that we do not know its equation\n",
    "    - a reference point y is chosen (the barycenter usualy)\n",
    "    - the idea is to take the edge pixels and then doing a voting process in which each edge pixel do vote for the position of the reference point y based on the gradient orientation of the edge pixels\n",
    "    - the AA in this case is another image with the same size as the input one set to 0\n",
    "    - it is not rotation invariant, so basically what we do is try all the possible quantized rotations and then cast vote to each one, which means that AA is 3D\n",
    "    - it is not scale invariant, so basically we try all possible scales and then cast vote for each one, so AA is 4D\n",
    "    - we can also use GHt with SIFT local invariant features instead of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
